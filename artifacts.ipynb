{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts for Pattern Recognition Project\n",
    "## Authors: Ekansh Bhatnagar, Meghana Chintalapati\n",
    "\n",
    "Install required packages mentioned in requirements.txt\n",
    "absl-py==2.1.0\n",
    "aiohappyeyeballs==2.4.3\n",
    "aiohttp==3.11.8\n",
    "aiosignal==1.3.1\n",
    "anyio==4.6.2.post1\n",
    "argon2-cffi==23.1.0\n",
    "argon2-cffi-bindings==21.2.0\n",
    "arrow==1.3.0\n",
    "asttokens==2.4.1\n",
    "astunparse==1.6.3\n",
    "async-lru==2.0.4\n",
    "async-timeout==5.0.1\n",
    "attrs==24.2.0\n",
    "babel==2.16.0\n",
    "beautifulsoup4==4.12.3\n",
    "bleach==6.2.0\n",
    "certifi==2024.8.30\n",
    "cffi==1.17.1\n",
    "charset-normalizer==3.4.0\n",
    "colorama==0.4.6\n",
    "comm==0.2.2\n",
    "contourpy==1.3.1\n",
    "cycler==0.12.1\n",
    "debugpy==1.8.9\n",
    "decorator==5.1.1\n",
    "defusedxml==0.7.1\n",
    "emnist==0.0\n",
    "exceptiongroup==1.2.2\n",
    "executing==2.1.0\n",
    "fastjsonschema==2.21.0\n",
    "filelock==3.16.1\n",
    "flatbuffers==24.3.25\n",
    "fonttools==4.55.0\n",
    "fqdn==1.5.1\n",
    "frozenlist==1.5.0\n",
    "fsspec==2024.10.0\n",
    "gast==0.6.0\n",
    "google-pasta==0.2.0\n",
    "grpcio==1.68.0\n",
    "h11==0.14.0\n",
    "h5py==3.12.1\n",
    "httpcore==1.0.7\n",
    "httpx==0.28.0\n",
    "idna==3.10\n",
    "ipykernel==6.29.5\n",
    "ipython==8.30.0\n",
    "isoduration==20.11.0\n",
    "jedi==0.19.2\n",
    "Jinja2==3.1.4\n",
    "joblib==1.4.2\n",
    "json5==0.10.0\n",
    "jsonpointer==3.0.0\n",
    "jsonschema==4.23.0\n",
    "jsonschema-specifications==2024.10.1\n",
    "jupyter-events==0.10.0\n",
    "jupyter-lsp==2.2.5\n",
    "jupyter_client==8.6.3\n",
    "jupyter_core==5.7.2\n",
    "jupyter_server==2.14.2\n",
    "jupyter_server_terminals==0.5.3\n",
    "jupyterlab==4.3.1\n",
    "jupyterlab_pygments==0.3.0\n",
    "jupyterlab_server==2.27.3\n",
    "keras==3.7.0\n",
    "kiwisolver==1.4.7\n",
    "libclang==18.1.1\n",
    "lightning-utilities==0.11.9\n",
    "Markdown==3.7\n",
    "markdown-it-py==3.0.0\n",
    "MarkupSafe==3.0.2\n",
    "matplotlib==3.9.2\n",
    "matplotlib-inline==0.1.7\n",
    "mdurl==0.1.2\n",
    "mistune==3.0.2\n",
    "ml-dtypes==0.4.1\n",
    "mpmath==1.3.0\n",
    "multidict==6.1.0\n",
    "namex==0.0.8\n",
    "nbclient==0.10.1\n",
    "nbconvert==7.16.4\n",
    "nbformat==5.10.4\n",
    "nest-asyncio==1.6.0\n",
    "networkx==3.4.2\n",
    "notebook_shim==0.2.4\n",
    "numpy==2.0.2\n",
    "opt_einsum==3.4.0\n",
    "optree==0.13.1\n",
    "overrides==7.7.0\n",
    "packaging==24.2\n",
    "pandas==2.2.3\n",
    "pandocfilters==1.5.1\n",
    "parso==0.8.4\n",
    "pillow==11.0.0\n",
    "platformdirs==4.3.6\n",
    "prometheus_client==0.21.0\n",
    "prompt_toolkit==3.0.48\n",
    "propcache==0.2.0\n",
    "protobuf==5.29.0\n",
    "psutil==6.1.0\n",
    "pure_eval==0.2.3\n",
    "pycparser==2.22\n",
    "Pygments==2.18.0\n",
    "pyparsing==3.2.0\n",
    "python-dateutil==2.9.0.post0\n",
    "python-json-logger==2.0.7\n",
    "python-version==0.0.2\n",
    "pytorch-lightning==2.4.0\n",
    "pytz==2024.2\n",
    "pywin32==308\n",
    "pywinpty==2.0.14\n",
    "PyYAML==6.0.2\n",
    "pyzmq==26.2.0\n",
    "referencing==0.35.1\n",
    "requests==2.32.3\n",
    "rfc3339-validator==0.1.4\n",
    "rfc3986-validator==0.1.1\n",
    "rich==13.9.4\n",
    "rpds-py==0.21.0\n",
    "scikit-learn==1.5.2\n",
    "scipy==1.14.1\n",
    "Send2Trash==1.8.3\n",
    "six==1.16.0\n",
    "sniffio==1.3.1\n",
    "soupsieve==2.6\n",
    "stack-data==0.6.3\n",
    "sympy==1.13.1\n",
    "tensorboard==2.18.0\n",
    "tensorboard-data-server==0.7.2\n",
    "tensorflow==2.18.0\n",
    "tensorflow-io-gcs-filesystem==0.31.0\n",
    "tensorflow_intel==2.18.0\n",
    "termcolor==2.5.0\n",
    "terminado==0.18.1\n",
    "threadpoolctl==3.5.0\n",
    "tinycss2==1.4.0\n",
    "tomli==2.2.1\n",
    "torch==2.5.1\n",
    "torchmetrics==1.6.0\n",
    "torchvision==0.20.1\n",
    "tornado==6.4.2\n",
    "tqdm==4.67.1\n",
    "traitlets==5.14.3\n",
    "types-python-dateutil==2.9.0.20241003\n",
    "typing_extensions==4.12.2\n",
    "tzdata==2024.2\n",
    "uri-template==1.3.0\n",
    "urllib3==2.2.3\n",
    "wcwidth==0.2.13\n",
    "webcolors==24.11.1\n",
    "webencodings==0.5.1\n",
    "websocket-client==1.8.0\n",
    "Werkzeug==3.1.3\n",
    "wrapt==1.17.0\n",
    "yarl==1.18.0\n",
    "\n",
    "## Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import ndimage\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\"\"\"This script defines the training, validation and testing process.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# SVM with polynomial kernel\n",
    "class SVM(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel='poly', degree=self.degree, coef0=1.0)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with translational-invariant kernel\n",
    "class TISVM(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def ti_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for k in range(-5, 6):\n",
    "            for l in range(-5, 6):\n",
    "                # shift\n",
    "                T_kl = np.roll(y_reshape, (k, l), axis=(1, 2))\n",
    "                T_kl_reshape = T_kl.reshape((T_kl.shape[0], -1))\n",
    "\n",
    "                kernel_medium = polynomial_kernel(x, T_kl_reshape, self.degree)\n",
    "                kernel_final = np.maximum(kernel_medium, kernel_final)\n",
    "\n",
    "        return kernel_final\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ti_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with translational-invariant kernel\n",
    "class TIRISVM(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def ti_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for k in range(-7, 8):\n",
    "            for l in range(-7, 8):\n",
    "                for d in range(-10, 10):\n",
    "                    # shift\n",
    "                    T_kl = np.roll(y_reshape, (k, l), axis=(1, 2))\n",
    "                    # rotate\n",
    "                    T_rotate = ndimage.rotate(T_kl, d, axes=(1, 2), reshape=False)\n",
    "                    T_kl_reshape = T_rotate.reshape((T_kl.shape[0], -1))\n",
    "\n",
    "                    kernel_medium = polynomial_kernel(x, T_kl_reshape, self.degree)\n",
    "                    kernel_final = np.maximum(kernel_medium, kernel_final)\n",
    "\n",
    "        return kernel_final\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ti_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with translational-invariant kernel\n",
    "class LTIRISVM(object):\n",
    "    def __init__(self, degree, filter):\n",
    "        self.degree = degree\n",
    "        self.filter = filter\n",
    "\n",
    "    def ti_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for k in range(-7, 8):\n",
    "            for l in range(-7, 8):\n",
    "                for d in range(-5, 6):\n",
    "                    # rotate\n",
    "                    T_rotate = ndimage.rotate(y_reshape, d, axes=(1, 2), reshape=False)\n",
    "                    # shift\n",
    "                    T_kl = np.roll(T_rotate, (k, l), axis=(1, 2))\n",
    "                    T_kl_reshape = T_kl.reshape((T_kl.shape[0], -1))\n",
    "\n",
    "                    kernel_medium = self.locality_kernel(x, T_kl_reshape)\n",
    "                    kernel_final = np.maximum(kernel_medium, kernel_final)\n",
    "\n",
    "        return kernel_final\n",
    "\n",
    "    # using im2col to accelerate computation (memory locality)\n",
    "    def im2col(self, x, filter_h, filter_w, stride=1, pad=0):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        assert (H + 2 * pad - filter_h) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Height'\n",
    "        assert (W + 2 * pad - filter_w) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Width'\n",
    "        out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "        out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "        img = np.pad(x, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, out_h, out_w, filter_h, filter_w))\n",
    "\n",
    "        for y in range(out_h):\n",
    "            for x in range(out_w):\n",
    "                col[:, :, y, x, :, :] = img[:, :, y * stride:y * stride + filter_h, x * stride:x * stride + filter_w]\n",
    "\n",
    "        col = col.transpose(0, 2, 3, 4, 5, 1).reshape((N, -1))\n",
    "        return col\n",
    "\n",
    "    def locality_kernel(self, x, y):\n",
    "        X1 = np.transpose(x.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "        Y1 = np.transpose(y.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "\n",
    "        Z = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        # using im2col to accelerate computation (memory locality)\n",
    "        filter_h = self.filter\n",
    "        filter_w = self.filter\n",
    "        stride = 1\n",
    "        pad = (self.filter - 1) // 2\n",
    "        X2 = self.im2col(X1, filter_h, filter_w, stride, pad)\n",
    "        Y2 = self.im2col(Y1, filter_h, filter_w, stride, pad)\n",
    "\n",
    "        # Hyperparameter: Degree\n",
    "        D1 = 2\n",
    "        D2 = self.degree // D1\n",
    "\n",
    "        for i in range(0, X2.shape[1], filter_h * filter_w):\n",
    "            Z = Z + polynomial_kernel(X2[:, i: i + filter_h * filter_w], Y2[:, i: i + filter_h * filter_w],\n",
    "                                      degree=D1, coef0=1.0)\n",
    "\n",
    "        Z = (1 / (X1.shape[1] * X1.shape[2]) * Z + np.ones((x.shape[0], y.shape[0]))) ** D2\n",
    "        return Z\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ti_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with translational-invariant kernel\n",
    "class LTISVM(object):\n",
    "    def __init__(self, degree, filter):\n",
    "        self.degree = degree\n",
    "        self.filter = filter\n",
    "\n",
    "    def ti_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for k in range(-7, 8):\n",
    "            for l in range(-7, 8):\n",
    "                # shift\n",
    "                T_kl = np.roll(y_reshape, (k, l), axis=(1, 2))\n",
    "                T_kl_reshape = T_kl.reshape((T_kl.shape[0], -1))\n",
    "\n",
    "                kernel_medium = self.locality_kernel(x, T_kl_reshape)\n",
    "                kernel_final = np.maximum(kernel_medium, kernel_final)\n",
    "\n",
    "        return kernel_final\n",
    "\n",
    "    # using im2col to accelerate computation (memory locality)\n",
    "    def im2col(self, x, filter_h, filter_w, stride=1, pad=0):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        assert (H + 2 * pad - filter_h) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Height'\n",
    "        assert (W + 2 * pad - filter_w) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Width'\n",
    "        out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "        out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "        img = np.pad(x, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, out_h, out_w, filter_h, filter_w))\n",
    "\n",
    "        for y in range(out_h):\n",
    "            for x in range(out_w):\n",
    "                col[:, :, y, x, :, :] = img[:, :, y * stride:y * stride + filter_h, x * stride:x * stride + filter_w]\n",
    "\n",
    "        col = col.transpose(0, 2, 3, 4, 5, 1).reshape((N, -1))\n",
    "        return col\n",
    "\n",
    "    def locality_kernel(self, x, y):\n",
    "        X1 = np.transpose(x.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "        Y1 = np.transpose(y.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "\n",
    "        Z = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        # using im2col to accelerate computation (memory locality)\n",
    "        filter_h = self.filter\n",
    "        filter_w = self.filter\n",
    "        stride = 1\n",
    "        pad = (self.filter - 1) // 2\n",
    "        X2 = self.im2col(X1, filter_h, filter_w, stride, pad)\n",
    "        Y2 = self.im2col(Y1, filter_h, filter_w, stride, pad)\n",
    "\n",
    "        # Hyperparameter: Degree\n",
    "        D1 = 2\n",
    "        D2 = self.degree // D1\n",
    "\n",
    "        for i in range(0, X2.shape[1], filter_h * filter_w):\n",
    "            Z = Z + polynomial_kernel(X2[:, i: i + filter_h * filter_w], Y2[:, i: i + filter_h * filter_w],\n",
    "                                      degree=D1, coef0=1.0)\n",
    "\n",
    "        Z = (1 / (X1.shape[1] * X1.shape[2]) * Z + np.ones((x.shape[0], y.shape[0]))) ** D2\n",
    "        return Z\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ti_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with local connective kernel\n",
    "class LOCSVM(object):\n",
    "    def __init__(self, degree, filter):\n",
    "        self.degree = degree\n",
    "        self.filter = filter\n",
    "\n",
    "    # using im2col to accelerate computation (memory locality)\n",
    "    def im2col(self, x, filter_h, filter_w, stride=1, pad=0):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        assert (H + 2 * pad - filter_h) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Height'\n",
    "        assert (W + 2 * pad - filter_w) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Width'\n",
    "        out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "        out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "        img = np.pad(x, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, out_h, out_w, filter_h, filter_w))\n",
    "\n",
    "        for y in range(out_h):\n",
    "            for x in range(out_w):\n",
    "                col[:, :, y, x, :, :] = img[:, :, y * stride:y * stride + filter_h, x * stride:x * stride + filter_w]\n",
    "\n",
    "        col = col.transpose(0, 2, 3, 4, 5, 1).reshape((N, -1))\n",
    "        return col\n",
    "\n",
    "    def locality_kernel(self, x, y):\n",
    "        X1 = np.transpose(x.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "        Y1 = np.transpose(y.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "\n",
    "        Z = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        # using im2col to accelerate computation (memory locality)\n",
    "        filter_h = self.filter\n",
    "        filter_w = self.filter\n",
    "        stride = 1\n",
    "        pad = (self.filter - 1) // 2\n",
    "        X2 = self.im2col(X1, filter_h, filter_w, stride, pad)\n",
    "        Y2 = self.im2col(Y1, filter_h, filter_w, stride, pad)\n",
    "\n",
    "        # Hyperparameter: Degree\n",
    "        D1 = 2\n",
    "        D2 = self.degree // D1\n",
    "\n",
    "        for i in range(0, X2.shape[1], filter_h * filter_w):\n",
    "            Z = Z + polynomial_kernel(X2[:, i: i + filter_h * filter_w], Y2[:, i: i + filter_h * filter_w],\n",
    "                                      degree=D1, coef0=1.0)\n",
    "\n",
    "        Z = (1 / (X1.shape[1] * X1.shape[2]) * Z + np.ones((x.shape[0], y.shape[0]))) ** D2\n",
    "        return Z\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.locality_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with rotation-invariant kernel\n",
    "class RISVM(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def ri_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for d in range(-10, 10):\n",
    "            # rotate\n",
    "            T_rotate = ndimage.rotate(y_reshape, d, axes=(1, 2), reshape=False)\n",
    "            T_rotate_reshape = T_rotate.reshape((T_rotate.shape[0], -1))\n",
    "\n",
    "            kernel_medium = polynomial_kernel(x, T_rotate_reshape, self.degree)\n",
    "            kernel_final = np.maximum(kernel_medium, kernel_final)\n",
    "\n",
    "        return kernel_final\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ri_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with rotation-invariant kernel\n",
    "class RIISVM(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def ri_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for d in range(-10, 10):\n",
    "            # rotate\n",
    "            T_rotate = ndimage.rotate(y_reshape, d, axes=(1, 2), reshape=False)\n",
    "            T_rotate_reshape = T_rotate.reshape((T_rotate.shape[0], -1))\n",
    "\n",
    "            kernel_final = kernel_final + polynomial_kernel(x, T_rotate_reshape, self.degree)\n",
    "\n",
    "        return kernel_final/20\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ri_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with rotation-invariant kernel\n",
    "class LRISVM(object):\n",
    "    def __init__(self, degree, filter):\n",
    "        self.degree = degree\n",
    "        self.filter = filter\n",
    "\n",
    "    def ri_kernel(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: arrays of shape (n_samples1, n_features)\n",
    "            y: arrays of shape (n_samples2, n_features)\n",
    "        Returns:\n",
    "            kernel_final: maximum kernel matrix of shape (n_samples1, n_samples2)\n",
    "        \"\"\"\n",
    "        y_reshape = y.reshape((-1, 28, 28))\n",
    "\n",
    "        # choose the maximum kernel for each pair of sample\n",
    "        kernel_final = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        for d in range(-5, 6):\n",
    "            # rotate\n",
    "            T_rotate = ndimage.rotate(y_reshape, d, axes=(1, 2), reshape=False)\n",
    "            T_rotate_reshape = T_rotate.reshape((T_rotate.shape[0], -1))\n",
    "\n",
    "            kernel_medium = self.locality_kernel(x, T_rotate_reshape)\n",
    "            kernel_final = np.maximum(kernel_medium, kernel_final)\n",
    "\n",
    "        return kernel_final\n",
    "\n",
    "    # using im2col to accelerate computation (memory locality)\n",
    "    def im2col(self, x, filter_h, filter_w, stride=1, pad=0):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        assert (H + 2 * pad - filter_h) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Height'\n",
    "        assert (W + 2 * pad - filter_w) % stride == 0, 'Sanity Check Status: Conv Layer Failed in Width'\n",
    "        out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "        out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "        img = np.pad(x, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, out_h, out_w, filter_h, filter_w))\n",
    "\n",
    "        for y in range(out_h):\n",
    "            for x in range(out_w):\n",
    "                col[:, :, y, x, :, :] = img[:, :, y * stride:y * stride + filter_h, x * stride:x * stride + filter_w]\n",
    "\n",
    "        col = col.transpose(0, 2, 3, 4, 5, 1).reshape((N, -1))\n",
    "        return col\n",
    "\n",
    "    def locality_kernel(self, x, y):\n",
    "        X1 = np.transpose(x.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "        Y1 = np.transpose(y.reshape((-1, 28, 28, 1)), (0, 3, 1, 2))\n",
    "\n",
    "        Z = np.zeros((x.shape[0], y.shape[0]))\n",
    "\n",
    "        # using im2col to accelerate computation (memory locality)\n",
    "        filter_h = self.filter\n",
    "        filter_w = self.filter\n",
    "        stride = 1\n",
    "        pad = (self.filter - 1) // 2\n",
    "        X2 = self.im2col(X1, filter_h, filter_w, stride, pad)\n",
    "        Y2 = self.im2col(Y1, filter_h, filter_w, stride, pad)\n",
    "\n",
    "        # Hyperparameter: Degree\n",
    "        D1 = 2\n",
    "        D2 = self.degree // D1\n",
    "\n",
    "        for i in range(0, X2.shape[1], filter_h * filter_w):\n",
    "            Z = Z + polynomial_kernel(X2[:, i: i + filter_h * filter_w], Y2[:, i: i + filter_h * filter_w],\n",
    "                                      degree=D1, coef0=1.0)\n",
    "\n",
    "        Z = (1 / (X1.shape[1] * X1.shape[2]) * Z + np.ones((x.shape[0], y.shape[0]))) ** D2\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma='auto', kernel=self.ri_kernel)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# kNN with tangent distance\n",
    "class KNN(object):\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def train(self, x, y):\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3, metric=self.metrics)\n",
    "        neigh.fit(x, y)\n",
    "        y_pred = neigh.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return neigh, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, neigh):\n",
    "        y_pred = neigh.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# SVM with tangent distance\n",
    "class TDSVM(object):\n",
    "    def __init__(self, metrics):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def TD(self, x, y):\n",
    "        return np.exp(-pairwise_distances(x, y, metric=self.metrics)**2 / 784)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        svclassifier = SVC(gamma=1, kernel=self.TD)\n",
    "        svclassifier.fit(x, y)\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        train_acc = accuracy_score(y, y_pred)\n",
    "        return svclassifier, train_acc\n",
    "\n",
    "    def evaluate(self, x, y, svclassifier):\n",
    "        y_pred = svclassifier.predict(x)\n",
    "        eval_acc = accuracy_score(y, y_pred)\n",
    "        return eval_acc\n",
    "\n",
    "\n",
    "# ResNet\n",
    "class ResNetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(num_classes=10)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_no):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run for TB dataset run the code in next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from Model import SVM, TISVM, RISVM, LOCSVM, LTISVM, LRISVM, TIRISVM, LTIRISVM, RIISVM, KNN, TDSVM, ResNetModel\n",
    "from DataLoader import load_data, train_valid_split\n",
    "import ctypes\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_images(image_paths):\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = os.path.join(image_dir, img_path)\n",
    "        img = load_img(full_path, target_size=(224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "    \n",
    "def preprocess_images_svm(image_paths):\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = os.path.join(image_dir, img_path)\n",
    "        img = load_img(full_path, target_size=(28, 28),color_mode='grayscale')  # Resize for simplicity\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = img_array.flatten() / 255.0  # Flatten and normalize\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'TB/tb_metadata.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "samples_num = [100, 200, 400]\n",
    "results = open(\"result.txt\",\"w\",buffering=1)\n",
    "results.write(\"model,number_of_samples,test_accuracy\\n\")\n",
    "\n",
    "for num in samples_num:\n",
    "    # Select the first 100 entries\n",
    "    subset_df = df[['PATH', 'Label']].head(num)\n",
    "\n",
    "    # Get the lists of image paths and labels from the sample\n",
    "    sample_image_paths = subset_df['PATH'].tolist()\n",
    "    sample_labels = subset_df['Label'].tolist()\n",
    "\n",
    "    test_set_df = df[['PATH', 'Label']].iloc[num:num+500]\n",
    "    test_image_paths = test_set_df['PATH'].tolist()\n",
    "    test_labels = test_set_df['Label'].tolist()\n",
    "\n",
    "    # Define image directory\n",
    "    image_dir = 'TB/images/'\n",
    "\n",
    "    X = preprocess_images(sample_image_paths)\n",
    "    X_test = preprocess_images(test_image_paths)\n",
    "\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['Label'])\n",
    "    y_encoded = label_encoder.transform(sample_labels)\n",
    "    y_test_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    y_test_categorical = to_categorical(y_test_encoded)\n",
    "    input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = ResNet50(weights=None, include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    #Adjust the number of classes according to your dataset\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, y_categorical, batch_size=16, epochs=10, validation_split=0.2)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
    "\n",
    "    print(\"RESNET50:\")\n",
    "    print(f'Test Accuracy with {num} samples: {accuracy * 100:.2f}%')\n",
    "    results.write(\"RESNET50,\"+str(num)+\",\"+str(accuracy * 100)+\"\\n\")\n",
    "\n",
    "    X = preprocess_images_svm(sample_image_paths)\n",
    "    X_test = preprocess_images_svm(test_image_paths)\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['Label'])\n",
    "    y_encoded = label_encoder.transform(sample_labels)\n",
    "    y_test_encoded = label_encoder.transform(test_labels)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    y_test_categorical = to_categorical(y_test_encoded)\n",
    "    \n",
    "    model_svm = SVM(degree=8)\n",
    "    model_tisvm = TISVM(degree=8)\n",
    "    model_locsvm = LOCSVM(degree=9, filter=9)\n",
    "    model_lrisvm = LRISVM(degree=8, filter=3)\n",
    "    model_ltisvm = LTISVM(degree=8,filter=7)\n",
    "    model_risvm = RISVM(degree=8)\n",
    "    model_riisvm = RIISVM(degree=8)\n",
    "    model_tirisvm = TIRISVM(degree=8)\n",
    "    model_ltirisvm = LTIRISVM(degree=8,filter=7)\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_svm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_svm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print(\"SVM (\",time2-time2,\"}s):\")\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"SVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_tisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_tisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('TISVM (',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"TISVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_locsvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_locsvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LOCSVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LOCSVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_lrisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_lrisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LRISVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LRISVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_ltisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_ltisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LTISVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LTISVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    #model_ltirisvm\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_ltirisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_ltirisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LTIRISVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LTIRSVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run MICROGRAPH Dataset run next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from Model import SVM, TISVM, RISVM, LOCSVM, LTISVM, LRISVM, TIRISVM, LTIRISVM, RIISVM, KNN, TDSVM, ResNetModel\n",
    "from DataLoader import load_data, train_valid_split\n",
    "import ctypes\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_images(image_paths):\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = os.path.join(image_dir, img_path)\n",
    "        img = load_img(full_path, target_size=(224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "    \n",
    "def preprocess_images_svm(image_paths):\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = os.path.join(image_dir, img_path)\n",
    "        img = load_img(full_path, target_size=(28, 28),color_mode='grayscale')  # Resize for simplicity\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = img_array.flatten() / 255.0  # Flatten and normalize\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'micrograph.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "samples_num = [100, 200, 400]\n",
    "results = open(\"result.txt\",\"w\",buffering=1)\n",
    "results.write(\"model,number_of_samples,test_accuracy\\n\")\n",
    "\n",
    "for num in samples_num:\n",
    "    # Select the first 100 entries\n",
    "    subset_df = df[['path', 'primary_microconstituent']].head(num)\n",
    "\n",
    "    # Get the lists of image paths and labels from the sample\n",
    "    sample_image_paths = subset_df['path'].tolist()\n",
    "    sample_labels = subset_df['primary_microconstituent'].tolist()\n",
    "\n",
    "    test_set_df = df[['path', 'primary_microconstituent']].iloc[num:num+500]\n",
    "    test_image_paths = test_set_df['path'].tolist()\n",
    "    test_labels = test_set_df['primary_microconstituent'].tolist()\n",
    "\n",
    "    # Define image directory\n",
    "    image_dir = 'images/'\n",
    "\n",
    "    X = preprocess_images(sample_image_paths)\n",
    "    X_test = preprocess_images(test_image_paths)\n",
    "\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['primary_microconstituent'])\n",
    "    y_encoded = label_encoder.transform(sample_labels)\n",
    "    y_test_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    y_test_categorical = to_categorical(y_test_encoded)\n",
    "    input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = ResNet50(weights=None, include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    #Adjust the number of classes according to your dataset\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, y_categorical, batch_size=16, epochs=10, validation_split=0.2)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
    "\n",
    "    print(\"RESNET50:\")\n",
    "    print(f'Test Accuracy with {num} samples: {accuracy * 100:.2f}%')\n",
    "    results.write(\"RESNET50,\"+str(num)+\",\"+str(accuracy * 100)+\"\\n\")\n",
    "\n",
    "    X = preprocess_images_svm(sample_image_paths)\n",
    "    X_test = preprocess_images_svm(test_image_paths)\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['primary_microconstituent'])\n",
    "    y_encoded = label_encoder.transform(sample_labels)\n",
    "    y_test_encoded = label_encoder.transform(test_labels)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    y_test_categorical = to_categorical(y_test_encoded)\n",
    "    \n",
    "    model_svm = SVM(degree=8)\n",
    "    model_tisvm = TISVM(degree=8)\n",
    "    model_locsvm = LOCSVM(degree=9, filter=9)\n",
    "    model_lrisvm = LRISVM(degree=8, filter=3)\n",
    "    model_ltisvm = LTISVM(degree=8,filter=7)\n",
    "    model_risvm = RISVM(degree=8)\n",
    "    model_riisvm = RIISVM(degree=8)\n",
    "    model_tirisvm = TIRISVM(degree=8)\n",
    "    model_ltirisvm = LTIRISVM(degree=8,filter=7)\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_svm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_svm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print(\"SVM (\",time2-time2,\"}s):\")\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"SVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_tisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_tisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('TISVM (',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"TISVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_locsvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_locsvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LOCSVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LOCSVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_lrisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_lrisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LRISVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LRISVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_ltisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_ltisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LTISVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LTISVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")\n",
    "\n",
    "    #model_ltirisvm\n",
    "\n",
    "    time1 = time.time()\n",
    "    svclassifier, train_acc = model_ltirisvm.train(X, y_encoded)\n",
    "    #eval_acc = model.evaluate(x_valid[0:val_list[idx], :], y_valid[0:val_list[idx]], svclassifier)\n",
    "    test_acc = model_ltirisvm.evaluate(X_test, y_test_encoded, svclassifier)\n",
    "    time2 = time.time()\n",
    "    print('LTIRISVM ((',time2-time2,'}s):')\n",
    "    print(f'Test Accuracy with {num} samples: {test_acc * 100:.2f}%')\n",
    "    results.write(\"LTIRSVM,\"+str(num)+\",\"+str(test_acc * 100)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
